{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy scikit-learn xgboost matplotlib seaborn joblib --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91507090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Configuration de l'affichage\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques charg√©es avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d12934",
   "metadata": {},
   "source": [
    "## üìÇ 1. Chargement et Exploration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset\n",
    "df = pd.read_csv('../ml/healthcare_dataset.csv')\n",
    "\n",
    "print(f\"üìä Dataset charg√©: {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
    "print(f\"üìÖ P√©riode: {df['date'].min()} √† {df['date'].max()}\")\n",
    "print(f\"\\nüè• Services: {df['service'].nunique()} uniques\")\n",
    "print(df['service'].unique())\n",
    "\n",
    "# Aper√ßu des donn√©es\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9772bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "print(\"üìà Statistiques descriptives:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification des valeurs manquantes\n",
    "print(\"üîç Valeurs manquantes:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"‚úÖ Aucune valeur manquante d√©tect√©e!\")\n",
    "else:\n",
    "    print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ccad1",
   "metadata": {},
   "source": [
    "## üìä 2. Visualisations Exploratoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4779e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvolution des co√ªts totaux par service\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df_pivot = df.pivot_table(values='cout_total', index='date', columns='service', aggfunc='sum')\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "for col in df_pivot.columns:\n",
    "    plt.plot(df_pivot.index, df_pivot[col], marker='o', label=col, linewidth=2)\n",
    "\n",
    "plt.title('üìà √âvolution des Co√ªts Totaux par Service (2024)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Co√ªt Total (‚Ç¨)', fontsize=12)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des co√ªts par service\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=df, x='service', y='cout_total', palette='Set2')\n",
    "plt.title('üì¶ Distribution des Co√ªts par Service', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Service', fontsize=12)\n",
    "plt.ylabel('Co√ªt Total (‚Ç¨)', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aea624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('üî• Matrice de Corr√©lation des Variables', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c4fe2",
   "metadata": {},
   "source": [
    "## üîß 3. Feature Engineering et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Encodage des variables cat√©gorielles\n",
    "le_service = LabelEncoder()\n",
    "le_saison = LabelEncoder()\n",
    "le_jour = LabelEncoder()\n",
    "le_meteo = LabelEncoder()\n",
    "\n",
    "df_processed['service_encoded'] = le_service.fit_transform(df_processed['service'])\n",
    "df_processed['saison_encoded'] = le_saison.fit_transform(df_processed['saison'])\n",
    "df_processed['jour_semaine_encoded'] = le_jour.fit_transform(df_processed['jour_semaine'])\n",
    "df_processed['meteo_encoded'] = le_meteo.fit_transform(df_processed['meteo'])\n",
    "\n",
    "# Features temporelles\n",
    "df_processed['jour_annee'] = df_processed['date'].dt.dayofyear\n",
    "df_processed['trimestre'] = df_processed['date'].dt.quarter\n",
    "\n",
    "# Features d'interaction\n",
    "df_processed['cout_par_patient'] = df_processed['cout_total'] / df_processed['patients_count']\n",
    "df_processed['actes_par_patient'] = df_processed['actes_count'] / df_processed['patients_count']\n",
    "df_processed['efficacite_personnel'] = df_processed['patients_count'] / df_processed['personnel_present']\n",
    "\n",
    "print(\"‚úÖ Feature Engineering termin√©!\")\n",
    "print(f\"üìä Nouvelles dimensions: {df_processed.shape}\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67202c2",
   "metadata": {},
   "source": [
    "## ü§ñ 4. Mod√®le 1: Pr√©diction des Co√ªts Totaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection des features pour la pr√©diction des co√ªts\n",
    "features_cout = [\n",
    "    'service_encoded', 'patients_count', 'actes_count', 'sejours_actifs',\n",
    "    'duree_moyenne_sejour', 'taux_occupation', 'personnel_present',\n",
    "    'equipements_utilises', 'urgences_admissions', 'interventions_chirurgicales',\n",
    "    'examens_radiologie', 'consultations', 'hospitalisations', 'tarif_moyen',\n",
    "    'saison_encoded', 'jour_semaine_encoded', 'est_weekend', 'est_ferie',\n",
    "    'meteo_encoded', 'temperature', 'mois', 'jour_annee', 'trimestre',\n",
    "    'actes_par_patient', 'efficacite_personnel'\n",
    "]\n",
    "\n",
    "X_cout = df_processed[features_cout]\n",
    "y_cout = df_processed['cout_total']\n",
    "\n",
    "# Split train/test\n",
    "X_train_cout, X_test_cout, y_train_cout, y_test_cout = train_test_split(\n",
    "    X_cout, y_cout, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalisation\n",
    "scaler_cout = StandardScaler()\n",
    "X_train_cout_scaled = scaler_cout.fit_transform(X_train_cout)\n",
    "X_test_cout_scaled = scaler_cout.transform(X_test_cout)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es pr√©par√©es pour pr√©diction des co√ªts\")\n",
    "print(f\"üìä Train: {X_train_cout.shape}, Test: {X_test_cout.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d175103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement de plusieurs mod√®les\n",
    "models_cout = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, max_depth=7, learning_rate=0.1, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=200, max_depth=7, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results_cout = {}\n",
    "\n",
    "print(\"üöÄ Entra√Ænement des mod√®les...\\n\")\n",
    "for name, model in models_cout.items():\n",
    "    print(f\"‚è≥ Entra√Ænement: {name}\")\n",
    "    model.fit(X_train_cout_scaled, y_train_cout)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred_train = model.predict(X_train_cout_scaled)\n",
    "    y_pred_test = model.predict(X_test_cout_scaled)\n",
    "    \n",
    "    # M√©triques\n",
    "    r2_train = r2_score(y_train_cout, y_pred_train)\n",
    "    r2_test = r2_score(y_test_cout, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test_cout, y_pred_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_cout, y_pred_test))\n",
    "    \n",
    "    results_cout[name] = {\n",
    "        'model': model,\n",
    "        'r2_train': r2_train,\n",
    "        'r2_test': r2_test,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name}:\")\n",
    "    print(f\"   R¬≤ Train: {r2_train:.4f}\")\n",
    "    print(f\"   R¬≤ Test: {r2_test:.4f}\")\n",
    "    print(f\"   MAE: {mae:.2f}‚Ç¨\")\n",
    "    print(f\"   RMSE: {rmse:.2f}‚Ç¨\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fff98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des mod√®les\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Mod√®le': list(results_cout.keys()),\n",
    "    'R¬≤ Train': [r['r2_train'] for r in results_cout.values()],\n",
    "    'R¬≤ Test': [r['r2_test'] for r in results_cout.values()],\n",
    "    'MAE (‚Ç¨)': [r['mae'] for r in results_cout.values()],\n",
    "    'RMSE (‚Ç¨)': [r['rmse'] for r in results_cout.values()]\n",
    "})\n",
    "\n",
    "print(\"üìä Comparaison des Mod√®les de Pr√©diction des Co√ªts:\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# S√©lection du meilleur mod√®le\n",
    "best_model_name = comparison_df.loc[comparison_df['R¬≤ Test'].idxmax(), 'Mod√®le']\n",
    "best_model_cout = results_cout[best_model_name]['model']\n",
    "print(f\"\\nüèÜ Meilleur mod√®le: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des pr√©dictions vs r√©el\n",
    "y_pred_best = results_cout[best_model_name]['predictions']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test_cout, y_pred_best, alpha=0.6, s=80, edgecolors='k')\n",
    "plt.plot([y_test_cout.min(), y_test_cout.max()], \n",
    "         [y_test_cout.min(), y_test_cout.max()], \n",
    "         'r--', lw=3, label='Pr√©diction parfaite')\n",
    "plt.xlabel('Co√ªts R√©els (‚Ç¨)', fontsize=12)\n",
    "plt.ylabel('Co√ªts Pr√©dits (‚Ç¨)', fontsize=12)\n",
    "plt.title(f'üéØ Pr√©diction vs R√©el - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features_cout,\n",
    "        'importance': best_model_cout.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "    plt.title(f'üîù Top 15 Features les plus importantes - {best_model_name}', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b0282",
   "metadata": {},
   "source": [
    "## ü§ñ 5. Mod√®le 2: Pr√©diction du Nombre de Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39abb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features pour pr√©diction des patients\n",
    "features_patients = [\n",
    "    'service_encoded', 'taux_occupation', 'personnel_present',\n",
    "    'equipements_utilises', 'saison_encoded', 'jour_semaine_encoded',\n",
    "    'est_weekend', 'est_ferie', 'meteo_encoded', 'temperature',\n",
    "    'mois', 'jour_annee', 'trimestre'\n",
    "]\n",
    "\n",
    "X_patients = df_processed[features_patients]\n",
    "y_patients = df_processed['patients_count']\n",
    "\n",
    "# Split et normalisation\n",
    "X_train_pat, X_test_pat, y_train_pat, y_test_pat = train_test_split(\n",
    "    X_patients, y_patients, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_patients = StandardScaler()\n",
    "X_train_pat_scaled = scaler_patients.fit_transform(X_train_pat)\n",
    "X_test_pat_scaled = scaler_patients.transform(X_test_pat)\n",
    "\n",
    "# Entra√Ænement XGBoost (meilleur mod√®le)\n",
    "model_patients = XGBRegressor(n_estimators=200, max_depth=7, learning_rate=0.1, \n",
    "                               random_state=42, n_jobs=-1)\n",
    "model_patients.fit(X_train_pat_scaled, y_train_pat)\n",
    "\n",
    "# √âvaluation\n",
    "y_pred_pat = model_patients.predict(X_test_pat_scaled)\n",
    "r2_pat = r2_score(y_test_pat, y_pred_pat)\n",
    "mae_pat = mean_absolute_error(y_test_pat, y_pred_pat)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le Pr√©diction Patients:\")\n",
    "print(f\"   R¬≤ Score: {r2_pat:.4f}\")\n",
    "print(f\"   MAE: {mae_pat:.2f} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986cd8b",
   "metadata": {},
   "source": [
    "## ü§ñ 6. Mod√®le 3: Pr√©diction du Taux d'Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features pour pr√©diction du taux d'occupation\n",
    "features_occupation = [\n",
    "    'service_encoded', 'patients_count', 'sejours_actifs', 'personnel_present',\n",
    "    'saison_encoded', 'jour_semaine_encoded', 'est_weekend', 'est_ferie',\n",
    "    'mois', 'trimestre'\n",
    "]\n",
    "\n",
    "X_occupation = df_processed[features_occupation]\n",
    "y_occupation = df_processed['taux_occupation']\n",
    "\n",
    "# Split et normalisation\n",
    "X_train_occ, X_test_occ, y_train_occ, y_test_occ = train_test_split(\n",
    "    X_occupation, y_occupation, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_occupation = StandardScaler()\n",
    "X_train_occ_scaled = scaler_occupation.fit_transform(X_train_occ)\n",
    "X_test_occ_scaled = scaler_occupation.transform(X_test_occ)\n",
    "\n",
    "# Entra√Ænement\n",
    "model_occupation = XGBRegressor(n_estimators=200, max_depth=7, learning_rate=0.1,\n",
    "                                 random_state=42, n_jobs=-1)\n",
    "model_occupation.fit(X_train_occ_scaled, y_train_occ)\n",
    "\n",
    "# √âvaluation\n",
    "y_pred_occ = model_occupation.predict(X_test_occ_scaled)\n",
    "r2_occ = r2_score(y_test_occ, y_pred_occ)\n",
    "mae_occ = mean_absolute_error(y_test_occ, y_pred_occ)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le Pr√©diction Taux d'Occupation:\")\n",
    "print(f\"   R¬≤ Score: {r2_occ:.4f}\")\n",
    "print(f\"   MAE: {mae_occ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d2f4d",
   "metadata": {},
   "source": [
    "## üíæ 7. Sauvegarde des Mod√®les et Encodeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ab263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des mod√®les\n",
    "import os\n",
    "\n",
    "models_dir = '../ml/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Sauvegarder les mod√®les\n",
    "joblib.dump(best_model_cout, f'{models_dir}/model_cout.pkl')\n",
    "joblib.dump(model_patients, f'{models_dir}/model_patients.pkl')\n",
    "joblib.dump(model_occupation, f'{models_dir}/model_occupation.pkl')\n",
    "\n",
    "# Sauvegarder les scalers\n",
    "joblib.dump(scaler_cout, f'{models_dir}/scaler_cout.pkl')\n",
    "joblib.dump(scaler_patients, f'{models_dir}/scaler_patients.pkl')\n",
    "joblib.dump(scaler_occupation, f'{models_dir}/scaler_occupation.pkl')\n",
    "\n",
    "# Sauvegarder les encodeurs\n",
    "encoders = {\n",
    "    'service': le_service,\n",
    "    'saison': le_saison,\n",
    "    'jour_semaine': le_jour,\n",
    "    'meteo': le_meteo\n",
    "}\n",
    "joblib.dump(encoders, f'{models_dir}/encoders.pkl')\n",
    "\n",
    "# Sauvegarder les features\n",
    "features_info = {\n",
    "    'cout': features_cout,\n",
    "    'patients': features_patients,\n",
    "    'occupation': features_occupation\n",
    "}\n",
    "joblib.dump(features_info, f'{models_dir}/features_info.pkl')\n",
    "\n",
    "print(\"‚úÖ Tous les mod√®les et artefacts sauvegard√©s dans:\", models_dir)\n",
    "print(\"\\nüì¶ Fichiers cr√©√©s:\")\n",
    "for file in os.listdir(models_dir):\n",
    "    print(f\"   - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6a8a6",
   "metadata": {},
   "source": [
    "## üîÆ 8. G√©n√©ration de Pr√©dictions Futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75158a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour cr√©er des donn√©es futures\n",
    "def create_future_data(service_name, days_ahead=30):\n",
    "    \"\"\"\n",
    "    Cr√©e un dataframe de donn√©es futures pour les pr√©dictions\n",
    "    \"\"\"\n",
    "    # Date de d√©part\n",
    "    start_date = pd.to_datetime('2024-12-15')\n",
    "    dates = [start_date + timedelta(days=i) for i in range(days_ahead)]\n",
    "    \n",
    "    # Moyennes du service pour les features\n",
    "    service_data = df[df['service'] == service_name]\n",
    "    \n",
    "    future_data = []\n",
    "    for date in dates:\n",
    "        # Features basiques\n",
    "        row = {\n",
    "            'date': date,\n",
    "            'service': service_name,\n",
    "            'mois': date.month,\n",
    "            'jour_annee': date.dayofyear,\n",
    "            'trimestre': date.quarter,\n",
    "            'jour_semaine': date.strftime('%A').lower(),\n",
    "            'est_weekend': 1 if date.weekday() >= 5 else 0,\n",
    "            'est_ferie': 0,  # Simplification\n",
    "        }\n",
    "        \n",
    "        # Saison\n",
    "        if date.month in [12, 1, 2]:\n",
    "            row['saison'] = 'hiver'\n",
    "            row['temperature'] = np.random.randint(0, 8)\n",
    "        elif date.month in [3, 4, 5]:\n",
    "            row['saison'] = 'printemps'\n",
    "            row['temperature'] = np.random.randint(10, 20)\n",
    "        elif date.month in [6, 7, 8]:\n",
    "            row['saison'] = 'ete'\n",
    "            row['temperature'] = np.random.randint(20, 35)\n",
    "        else:\n",
    "            row['saison'] = 'automne'\n",
    "            row['temperature'] = np.random.randint(10, 18)\n",
    "        \n",
    "        # M√©teo al√©atoire\n",
    "        row['meteo'] = np.random.choice(['ensoleille', 'nuageux', 'pluie', 'neige'])\n",
    "        \n",
    "        # Moyennes historiques du service\n",
    "        row['patients_count'] = int(service_data['patients_count'].mean())\n",
    "        row['actes_count'] = int(service_data['actes_count'].mean())\n",
    "        row['sejours_actifs'] = int(service_data['sejours_actifs'].mean())\n",
    "        row['duree_moyenne_sejour'] = service_data['duree_moyenne_sejour'].mean()\n",
    "        row['taux_occupation'] = service_data['taux_occupation'].mean()\n",
    "        row['personnel_present'] = int(service_data['personnel_present'].mean())\n",
    "        row['equipements_utilises'] = int(service_data['equipements_utilises'].mean())\n",
    "        row['urgences_admissions'] = int(service_data['urgences_admissions'].mean())\n",
    "        row['interventions_chirurgicales'] = int(service_data['interventions_chirurgicales'].mean())\n",
    "        row['examens_radiologie'] = int(service_data['examens_radiologie'].mean())\n",
    "        row['consultations'] = int(service_data['consultations'].mean())\n",
    "        row['hospitalisations'] = int(service_data['hospitalisations'].mean())\n",
    "        row['tarif_moyen'] = service_data['tarif_moyen'].mean()\n",
    "        \n",
    "        future_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(future_data)\n",
    "\n",
    "# Test avec un service\n",
    "future_urgences = create_future_data('Urgences', days_ahead=30)\n",
    "print(\"‚úÖ Donn√©es futures g√©n√©r√©es pour les Urgences (30 prochains jours)\")\n",
    "future_urgences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de pr√©diction compl√®te\n",
    "def predict_future_costs(future_df):\n",
    "    \"\"\"\n",
    "    Pr√©dit les co√ªts futurs pour un dataframe de donn√©es futures\n",
    "    \"\"\"\n",
    "    # Feature Engineering\n",
    "    future_processed = future_df.copy()\n",
    "    \n",
    "    # Encodage\n",
    "    future_processed['service_encoded'] = le_service.transform(future_processed['service'])\n",
    "    future_processed['saison_encoded'] = le_saison.transform(future_processed['saison'])\n",
    "    future_processed['jour_semaine_encoded'] = le_jour.transform(future_processed['jour_semaine'])\n",
    "    future_processed['meteo_encoded'] = le_meteo.transform(future_processed['meteo'])\n",
    "    \n",
    "    # Features d√©riv√©es\n",
    "    future_processed['actes_par_patient'] = future_processed['actes_count'] / future_processed['patients_count']\n",
    "    future_processed['efficacite_personnel'] = future_processed['patients_count'] / future_processed['personnel_present']\n",
    "    \n",
    "    # Extraction des features\n",
    "    X_future = future_processed[features_cout]\n",
    "    \n",
    "    # Normalisation\n",
    "    X_future_scaled = scaler_cout.transform(X_future)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    predictions = best_model_cout.predict(X_future_scaled)\n",
    "    \n",
    "    # Ajouter les pr√©dictions\n",
    "    future_processed['cout_predit'] = predictions\n",
    "    \n",
    "    return future_processed\n",
    "\n",
    "# Pr√©dire pour les Urgences\n",
    "predictions_urgences = predict_future_costs(future_urgences)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(predictions_urgences['date'], predictions_urgences['cout_predit'], \n",
    "         marker='o', linewidth=2, markersize=6, color='#17A2A6', label='Co√ªt pr√©dit')\n",
    "plt.fill_between(predictions_urgences['date'], \n",
    "                 predictions_urgences['cout_predit'] * 0.9,\n",
    "                 predictions_urgences['cout_predit'] * 1.1,\n",
    "                 alpha=0.2, color='#17A2A6', label='Intervalle de confiance ¬±10%')\n",
    "plt.title('üîÆ Pr√©diction des Co√ªts - Service Urgences (30 prochains jours)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Co√ªt Pr√©dit (‚Ç¨)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Statistiques des Pr√©dictions:\")\n",
    "print(f\"   Co√ªt moyen pr√©dit: {predictions_urgences['cout_predit'].mean():.2f}‚Ç¨\")\n",
    "print(f\"   Co√ªt min pr√©dit: {predictions_urgences['cout_predit'].min():.2f}‚Ç¨\")\n",
    "print(f\"   Co√ªt max pr√©dit: {predictions_urgences['cout_predit'].max():.2f}‚Ç¨\")\n",
    "print(f\"   Co√ªt total pr√©vu (30j): {predictions_urgences['cout_predit'].sum():.2f}‚Ç¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions pour tous les services\n",
    "all_predictions = {}\n",
    "\n",
    "for service in df['service'].unique():\n",
    "    future_data = create_future_data(service, days_ahead=30)\n",
    "    predictions = predict_future_costs(future_data)\n",
    "    all_predictions[service] = predictions\n",
    "\n",
    "# Visualisation comparative\n",
    "plt.figure(figsize=(16, 8))\n",
    "colors = ['#3B82F6', '#10B981', '#8B5CF6', '#F59E0B', '#EF4444', '#EC4899', '#06B6D4', '#84CC16']\n",
    "\n",
    "for idx, (service, pred) in enumerate(all_predictions.items()):\n",
    "    plt.plot(pred['date'], pred['cout_predit'], \n",
    "             marker='o', linewidth=2, label=service, color=colors[idx % len(colors)])\n",
    "\n",
    "plt.title('üîÆ Pr√©dictions des Co√ªts par Service (30 prochains jours)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Co√ªt Pr√©dit (‚Ç¨)', fontsize=12)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau r√©capitulatif\n",
    "summary = []\n",
    "for service, pred in all_predictions.items():\n",
    "    summary.append({\n",
    "        'Service': service,\n",
    "        'Co√ªt Moyen (‚Ç¨)': f\"{pred['cout_predit'].mean():.2f}\",\n",
    "        'Co√ªt Total 30j (‚Ç¨)': f\"{pred['cout_predit'].sum():.2f}\",\n",
    "        'Min (‚Ç¨)': f\"{pred['cout_predit'].min():.2f}\",\n",
    "        'Max (‚Ç¨)': f\"{pred['cout_predit'].max():.2f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\nüìä R√©sum√© des Pr√©dictions par Service (30 prochains jours):\\n\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65684ad7",
   "metadata": {},
   "source": [
    "## üìà 9. M√©triques Finales et Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final de tous les mod√®les\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ R√âSUM√â FINAL DES MOD√àLES ML\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ MOD√àLE PR√âDICTION DES CO√õTS ({best_model_name})\")\n",
    "print(f\"   R¬≤ Score: {results_cout[best_model_name]['r2_test']:.4f}\")\n",
    "print(f\"   MAE: {results_cout[best_model_name]['mae']:.2f}‚Ç¨\")\n",
    "print(f\"   RMSE: {results_cout[best_model_name]['rmse']:.2f}‚Ç¨\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ MOD√àLE PR√âDICTION NOMBRE DE PATIENTS\")\n",
    "print(f\"   R¬≤ Score: {r2_pat:.4f}\")\n",
    "print(f\"   MAE: {mae_pat:.2f} patients\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ MOD√àLE PR√âDICTION TAUX D'OCCUPATION\")\n",
    "print(f\"   R¬≤ Score: {r2_occ:.4f}\")\n",
    "print(f\"   MAE: {mae_occ:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TOUS LES MOD√àLES SONT OP√âRATIONNELS ET SAUVEGARD√âS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüì¶ Fichiers sauvegard√©s:\")\n",
    "print(\"   - model_cout.pkl (Pr√©diction co√ªts)\")\n",
    "print(\"   - model_patients.pkl (Pr√©diction patients)\")\n",
    "print(\"   - model_occupation.pkl (Pr√©diction taux occupation)\")\n",
    "print(\"   - scaler_*.pkl (Normalisateurs)\")\n",
    "print(\"   - encoders.pkl (Encodeurs cat√©goriels)\")\n",
    "print(\"   - features_info.pkl (Information sur les features)\")\n",
    "\n",
    "print(\"\\nüöÄ Pr√™t pour l'int√©gration avec le backend Spring Boot!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1559de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© ex√©cutif avec visualisation\n",
    "summary_text = f\"\"\"\n",
    "{'='*80}\n",
    "üè• R√âSUM√â EX√âCUTIF - MOD√àLES ML HEALTHCARE DASHBOARD\n",
    "{'='*80}\n",
    "\n",
    "üìä DATASET:\n",
    "   ‚Ä¢ P√©riode analys√©e: {df['date'].min()} √† {df['date'].max()}\n",
    "   ‚Ä¢ Nombre total d'observations: {len(df):,}\n",
    "   ‚Ä¢ Services couverts: {df['service'].nunique()}\n",
    "   ‚Ä¢ Features utilis√©es: {len(features_cout)}\n",
    "\n",
    "üèÜ MEILLEUR MOD√àLE: {best_model_name}\n",
    "   ‚Ä¢ R¬≤ Score (Test): {results_cout[best_model_name]['r2_test']:.4f}\n",
    "   ‚Ä¢ MAE: {results_cout[best_model_name]['mae']:.2f}‚Ç¨\n",
    "   ‚Ä¢ RMSE: {results_cout[best_model_name]['rmse']:.2f}‚Ç¨\n",
    "   ‚Ä¢ Pr√©cision moyenne: {(1 - results_cout[best_model_name]['mae'] / y_test_cout.mean()) * 100:.2f}%\n",
    "\n",
    "üìà PERFORMANCE GLOBALE:\n",
    "   ‚Ä¢ Co√ªt moyen pr√©dit: {results_cout[best_model_name]['predictions'].mean():.2f}‚Ç¨\n",
    "   ‚Ä¢ Co√ªt moyen r√©el: {y_test_cout.mean():.2f}‚Ç¨\n",
    "   ‚Ä¢ Erreur relative moyenne: {(results_cout[best_model_name]['mae'] / y_test_cout.mean() * 100):.2f}%\n",
    "\n",
    "üéØ TOP 3 FEATURES LES PLUS IMPORTANTES:\n",
    "\"\"\"\n",
    "\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    top_features = pd.DataFrame({\n",
    "        'feature': features_cout,\n",
    "        'importance': best_model_cout.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(3)\n",
    "    \n",
    "    for idx, row in top_features.iterrows():\n",
    "        summary_text += f\"   {row.name + 1}. {row['feature']}: {row['importance']:.4f}\\n\"\n",
    "\n",
    "summary_text += f\"\"\"\n",
    "üí° RECOMMANDATIONS:\n",
    "   1. D√©ployer le mod√®le {best_model_name} en production\n",
    "   2. Mettre en place un monitoring continu des pr√©dictions\n",
    "   3. R√©-entra√Æner le mod√®le tous les 3 mois avec nouvelles donn√©es\n",
    "   4. Surveiller particuli√®rement les services avec MAE √©lev√©\n",
    "   5. Int√©grer les pr√©dictions dans le dashboard temps r√©el\n",
    "\n",
    "‚úÖ STATUT: Tous les mod√®les entra√Æn√©s et sauvegard√©s avec succ√®s!\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_text)\n",
    "\n",
    "# Sauvegarder le r√©sum√©\n",
    "with open('../ml/models/model_summary.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_text)\n",
    "    \n",
    "print(\"\\nüìù R√©sum√© sauvegard√© dans: ml/models/model_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdce6ee",
   "metadata": {},
   "source": [
    "## üéì 11. Conclusions et Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison visuelle des 3 mod√®les\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (name, result) in enumerate(results_cout.items()):\n",
    "    axes[idx].scatter(y_test_cout, result['predictions'], alpha=0.6, s=60, edgecolors='k')\n",
    "    axes[idx].plot([y_test_cout.min(), y_test_cout.max()], \n",
    "                   [y_test_cout.min(), y_test_cout.max()], \n",
    "                   'r--', lw=2)\n",
    "    axes[idx].set_title(f'{name}\\nR¬≤ = {result[\"r2_test\"]:.4f}', \n",
    "                        fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Co√ªts R√©els (‚Ç¨)', fontsize=11)\n",
    "    axes[idx].set_ylabel('Co√ªts Pr√©dits (‚Ç¨)', fontsize=11)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('üî¨ Comparaison des 3 Mod√®les ML', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau comparatif\n",
    "print(\"\\nüìä Tableau Comparatif des Mod√®les:\\n\")\n",
    "comparison_full = pd.DataFrame({\n",
    "    'Mod√®le': list(results_cout.keys()),\n",
    "    'R¬≤ Train': [f\"{r['r2_train']:.4f}\" for r in results_cout.values()],\n",
    "    'R¬≤ Test': [f\"{r['r2_test']:.4f}\" for r in results_cout.values()],\n",
    "    'MAE': [f\"{r['mae']:.2f}‚Ç¨\" for r in results_cout.values()],\n",
    "    'RMSE': [f\"{r['rmse']:.2f}‚Ç¨\" for r in results_cout.values()],\n",
    "    'Surapprentissage': [f\"{(r['r2_train'] - r['r2_test']):.4f}\" for r in results_cout.values()]\n",
    "})\n",
    "print(comparison_full.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la performance par service\n",
    "service_performance = []\n",
    "\n",
    "for service_code in df_processed['service_encoded'].unique():\n",
    "    service_mask = X_test_cout['service_encoded'] == service_code\n",
    "    if service_mask.sum() > 0:\n",
    "        y_test_service = y_test_cout[service_mask]\n",
    "        y_pred_service = results_cout[best_model_name]['predictions'][service_mask]\n",
    "        \n",
    "        mae = mean_absolute_error(y_test_service, y_pred_service)\n",
    "        r2 = r2_score(y_test_service, y_pred_service)\n",
    "        \n",
    "        service_name = df_processed[df_processed['service_encoded'] == service_code]['service'].iloc[0]\n",
    "        \n",
    "        service_performance.append({\n",
    "            'Service': service_name,\n",
    "            'MAE (‚Ç¨)': mae,\n",
    "            'R¬≤ Score': r2,\n",
    "            '√âchantillons': service_mask.sum()\n",
    "        })\n",
    "\n",
    "perf_df = pd.DataFrame(service_performance).sort_values('MAE (‚Ç¨)')\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# MAE par service\n",
    "axes[0].barh(perf_df['Service'], perf_df['MAE (‚Ç¨)'], color='coral')\n",
    "axes[0].set_title('üìä MAE par Service', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('MAE (‚Ç¨)', fontsize=12)\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# R¬≤ par service\n",
    "colors = ['green' if r2 > 0.8 else 'orange' if r2 > 0.6 else 'red' for r2 in perf_df['R¬≤ Score']]\n",
    "axes[1].barh(perf_df['Service'], perf_df['R¬≤ Score'], color=colors)\n",
    "axes[1].set_title('üìà R¬≤ Score par Service', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('R¬≤ Score', fontsize=12)\n",
    "axes[1].axvline(x=0.8, color='green', linestyle='--', linewidth=2, alpha=0.5, label='Excellent (>0.8)')\n",
    "axes[1].axvline(x=0.6, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='Bon (>0.6)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Performance du Mod√®le par Service:\\n\")\n",
    "print(perf_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518596a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curves - √âvaluation de l'apprentissage\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model_cout, X_train_cout_scaled, y_train_cout,\n",
    "    cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2, color='orange')\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', linewidth=2, \n",
    "         markersize=8, label='Score Train')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='orange', linewidth=2, \n",
    "         markersize=8, label='Score Validation')\n",
    "\n",
    "plt.title(f'üìö Learning Curves - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Taille du Set d\\'Entra√Ænement', fontsize=13)\n",
    "plt.ylabel('MAE (Mean Absolute Error)', fontsize=13)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Learning curves g√©n√©r√©es pour {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de l'importance des features avec SHAP (si disponible)\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Cr√©er un explainer SHAP pour XGBoost\n",
    "    if best_model_name == 'XGBoost':\n",
    "        explainer = shap.TreeExplainer(best_model_cout)\n",
    "        shap_values = explainer.shap_values(X_test_cout_scaled[:100])  # Limiter √† 100 √©chantillons\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test_cout.iloc[:100], \n",
    "                         feature_names=features_cout, show=False)\n",
    "        plt.title('üîç SHAP - Importance et Impact des Features', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Analyse SHAP compl√©t√©e!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è SHAP analysis disponible uniquement pour XGBoost\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Biblioth√®que SHAP non install√©e. Installer avec: pip install shap\")\n",
    "    print(\"üìä Utilisation de Feature Importance standard √† la place\")\n",
    "    \n",
    "    # Alternative: Feature importance standard\n",
    "    if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': features_cout,\n",
    "            'importance': best_model_cout.feature_importances_\n",
    "        }).sort_values('importance', ascending=True).tail(20)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.barh(feature_imp['feature'], feature_imp['importance'], color='teal')\n",
    "        plt.title('üîù Top 20 Features - Importance Standard', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Importance', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.grid(alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des erreurs de pr√©diction\n",
    "errors = y_test_cout - results_cout[best_model_name]['predictions']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Histogramme des erreurs\n",
    "axes[0, 0].hist(errors, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Erreur nulle')\n",
    "axes[0, 0].set_title('üìä Distribution des Erreurs de Pr√©diction', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Erreur (R√©el - Pr√©dit)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Fr√©quence', fontsize=12)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Q-Q Plot\n",
    "from scipy import stats\n",
    "stats.probplot(errors, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('üìà Q-Q Plot - Normalit√© des Erreurs', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Erreurs par service\n",
    "service_errors = pd.DataFrame({\n",
    "    'service': X_test_cout['service_encoded'],\n",
    "    'error': errors\n",
    "})\n",
    "service_names = df_processed.groupby('service_encoded')['service'].first()\n",
    "service_errors['service_name'] = service_errors['service'].map(service_names)\n",
    "\n",
    "axes[1, 0].boxplot([service_errors[service_errors['service_name'] == s]['error'].values \n",
    "                     for s in service_names.sort_values().unique()],\n",
    "                    labels=service_names.sort_values().unique())\n",
    "axes[1, 0].set_title('üì¶ Distribution des Erreurs par Service', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Service', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Erreur (‚Ç¨)', fontsize=12)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# 4. Erreurs absolues vs Valeurs pr√©dites\n",
    "axes[1, 1].scatter(results_cout[best_model_name]['predictions'], np.abs(errors), \n",
    "                   alpha=0.5, s=50, edgecolors='k')\n",
    "axes[1, 1].set_title('üéØ Erreur Absolue vs Pr√©diction', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Valeur Pr√©dite (‚Ç¨)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Erreur Absolue (‚Ç¨)', fontsize=12)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Statistiques des Erreurs:\")\n",
    "print(f\"   Erreur moyenne: {errors.mean():.2f}‚Ç¨\")\n",
    "print(f\"   √âcart-type: {errors.std():.2f}‚Ç¨\")\n",
    "print(f\"   Erreur m√©diane: {errors.median():.2f}‚Ç¨\")\n",
    "print(f\"   Erreur max: {errors.max():.2f}‚Ç¨\")\n",
    "print(f\"   Erreur min: {errors.min():.2f}‚Ç¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a67750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion pour l'√©valuation binaire\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Classification: Co√ªts √©lev√©s (au-dessus de la m√©diane) vs faibles\n",
    "y_pred_class = (results_cout[best_model_name]['predictions'] > y_test_cout.median()).astype(int)\n",
    "y_test_class = (y_test_cout > y_test_cout.median()).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, square=True,\n",
    "            xticklabels=['Co√ªt Faible', 'Co√ªt √âlev√©'],\n",
    "            yticklabels=['Co√ªt Faible', 'Co√ªt √âlev√©'])\n",
    "plt.title('üéØ Matrice de Confusion - Classification des Co√ªts', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Valeur R√©elle', fontsize=13)\n",
    "plt.xlabel('Valeur Pr√©dite', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Rapport de Classification:\\n\")\n",
    "print(classification_report(y_test_class, y_pred_class, \n",
    "                          target_names=['Co√ªt Faible', 'Co√ªt √âlev√©']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60767558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbe ROC pour √©valuer la qualit√© du mod√®le\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Cr√©er des classes binaires bas√©es sur si le co√ªt d√©passe la m√©diane\n",
    "y_test_binary = (y_test_cout > y_test_cout.median()).astype(int)\n",
    "y_pred_proba = (results_cout[best_model_name]['predictions'] > y_test_cout.median()).astype(int)\n",
    "\n",
    "# Pour avoir des probabilit√©s, utilisons les pr√©dictions normalis√©es\n",
    "y_scores = (results_cout[best_model_name]['predictions'] - results_cout[best_model_name]['predictions'].min()) / \\\n",
    "           (results_cout[best_model_name]['predictions'].max() - results_cout[best_model_name]['predictions'].min())\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'Courbe ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Al√©atoire')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=13)\n",
    "plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=13)\n",
    "plt.title(f'üìà Courbe ROC - {best_model_name}\\nPr√©diction Co√ªts √âlev√©s vs Faibles', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9abc2",
   "metadata": {},
   "source": [
    "## üìä 10. Visualisations Avanc√©es et Analyses ML"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
